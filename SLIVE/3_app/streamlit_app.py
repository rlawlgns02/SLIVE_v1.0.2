"""
í•œêµ­ì–´ ìˆ˜ì–´ í†µì—­ ì›¹ì•± (Streamlit)

ì‹¤ì‹œê°„ ì›¹ìº ì„ í†µí•´ ìˆ˜ì–´ë¥¼ ì¸ì‹í•˜ê³  í•œê¸€ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤.
"""

import streamlit as st
import cv2
import torch
import numpy as np
import mediapipe as mp
from collections import deque
import sys
import os
import json

# ê²½ë¡œ ì¶”ê°€
sys.path.append('../2_models/word_classifier')
from lstm_model import LSTMClassifier

# ==================== ì„¤ì • ====================
st.set_page_config(
    page_title="í•œêµ­ì–´ ìˆ˜ì–´ í†µì—­ê¸°",
    page_icon="ğŸ¤Ÿ",
    layout="wide"
)

# MediaPipe ì´ˆê¸°í™”
mp_hands = mp.solutions.hands
mp_draw = mp.solutions.drawing_utils

# ==================== ëª¨ë¸ ë¡œë“œ ====================
@st.cache_resource
def load_model():
    """í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
    checkpoint_path = "../5_checkpoints/best_word_model.pth"

    if not os.path.exists(checkpoint_path):
        st.error(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
        st.info("ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµí•´ì£¼ì„¸ìš”: python 4_training/train_word_model_improved.py")
        return None, None

    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))

    # ëª¨ë¸ ì´ˆê¸°í™”
    config = checkpoint['config']
    model = LSTMClassifier(
        input_size=config['input_size'],
        hidden_size=config['hidden_size'],
        num_layers=config['num_layers'],
        num_classes=config['num_classes'],
        dropout=config['dropout'],
        bidirectional=config['bidirectional']
    )

    # ê°€ì¤‘ì¹˜ ë¡œë“œ
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    # ë¼ë²¨ ë§µ
    label_map = checkpoint['label_map']

    st.success(f"âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! (ì •í™•ë„: {checkpoint['val_acc']:.2f}%)")

    return model, label_map

# ==================== í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ ====================
def extract_keypoints(hand_landmarks):
    """ì† ëœë“œë§ˆí¬ì—ì„œ í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ (84 features)"""
    keypoints = []
    for lm in hand_landmarks.landmark:
        keypoints.extend([lm.x, lm.y])  # x, yë§Œ ì‚¬ìš© (z ì œì™¸)
    return keypoints  # 21 * 2 = 42

def extract_both_hands(results):
    """ì–‘ì† í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ"""
    left_kps = [0.0] * 42
    right_kps = [0.0] * 42

    if results.multi_hand_landmarks and results.multi_handedness:
        for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):
            label = handedness.classification[0].label  # "Left" or "Right"
            kps = extract_keypoints(hand_landmarks)

            if label == "Left":
                left_kps = kps
            else:
                right_kps = kps

    return left_kps + right_kps  # 84 features

# ==================== ë©”ì¸ ì•± ====================
def main():
    st.title("ğŸ¤Ÿ í•œêµ­ì–´ ìˆ˜ì–´ í†µì—­ê¸°")
    st.markdown("---")

    # ì‚¬ì´ë“œë°” ì„¤ì •
    st.sidebar.title("âš™ï¸ ì„¤ì •")
    confidence_threshold = st.sidebar.slider(
        "ì‹ ë¢°ë„ ì„ê³„ê°’",
        min_value=0.0,
        max_value=1.0,
        value=0.7,
        step=0.05,
        help="ì´ ê°’ë³´ë‹¤ ë†’ì€ ì‹ ë¢°ë„ì¼ ë•Œë§Œ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤"
    )

    sequence_length = st.sidebar.slider(
        "ì‹œí€€ìŠ¤ ê¸¸ì´ (í”„ë ˆì„)",
        min_value=10,
        max_value=50,
        value=30,
        step=5,
        help="ìˆ˜ì–´ ì¸ì‹ì— ì‚¬ìš©í•  í”„ë ˆì„ ìˆ˜"
    )

    show_keypoints = st.sidebar.checkbox("í‚¤í¬ì¸íŠ¸ í‘œì‹œ", value=True)

    # ëª¨ë¸ ë¡œë“œ
    model, label_map = load_model()
    if model is None:
        st.stop()

    # ì»¬ëŸ¼ ë ˆì´ì•„ì›ƒ
    col1, col2 = st.columns([2, 1])

    with col1:
        st.subheader("ğŸ“¹ ì›¹ìº  í”¼ë“œ")
        video_placeholder = st.empty()

    with col2:
        st.subheader("ğŸ“ ì¸ì‹ ê²°ê³¼")
        result_placeholder = st.empty()
        confidence_placeholder = st.empty()

        st.subheader("ğŸ“œ íˆìŠ¤í† ë¦¬")
        history_placeholder = st.empty()

    # ì‹œì‘/ì¤‘ì§€ ë²„íŠ¼
    start_button = st.button("ğŸ¥ ì‹œì‘", type="primary")
    stop_button = st.button("â¹ï¸ ì¤‘ì§€")

    if start_button:
        st.session_state['running'] = True

    if stop_button:
        st.session_state['running'] = False

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'running' not in st.session_state:
        st.session_state['running'] = False

    if 'history' not in st.session_state:
        st.session_state['history'] = []

    # ì›¹ìº  ì‹¤í–‰
    if st.session_state['running']:
        cap = cv2.VideoCapture(0)
        hands = mp_hands.Hands(
            max_num_hands=2,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )

        # ì‹œí€€ìŠ¤ ë²„í¼
        sequence_buffer = deque(maxlen=sequence_length)

        try:
            while st.session_state['running']:
                ret, frame = cap.read()
                if not ret:
                    st.error("ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    break

                # BGR â†’ RGB ë³€í™˜
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

                # ì† ì¸ì‹
                results = hands.process(frame_rgb)

                # í‚¤í¬ì¸íŠ¸ ì¶”ì¶œ
                if results.multi_hand_landmarks:
                    # ì–‘ì† í‚¤í¬ì¸íŠ¸
                    keypoints = extract_both_hands(results)
                    sequence_buffer.append(keypoints)

                    # ì† ê·¸ë¦¬ê¸°
                    if show_keypoints:
                        for hand_landmarks in results.multi_hand_landmarks:
                            mp_draw.draw_landmarks(
                                frame_rgb,
                                hand_landmarks,
                                mp_hands.HAND_CONNECTIONS
                            )

                    # ì‹œí€€ìŠ¤ê°€ ì¶©ë¶„íˆ ìŒ“ì´ë©´ ì¶”ë¡ 
                    if len(sequence_buffer) == sequence_length:
                        # ì‹œí€€ìŠ¤ ì¤€ë¹„
                        seq = torch.tensor(list(sequence_buffer), dtype=torch.float32)
                        seq = seq.unsqueeze(0)  # (1, seq_len, 84)

                        # ì¶”ë¡ 
                        with torch.no_grad():
                            pred = model(seq)
                            probs = torch.softmax(pred, dim=1)
                            confidence, predicted_idx = torch.max(probs, 1)

                        confidence_val = confidence.item()
                        predicted_idx_val = predicted_idx.item()

                        # ê²°ê³¼ í‘œì‹œ (ì„ê³„ê°’ ì´ìƒì¼ ë•Œë§Œ)
                        if confidence_val >= confidence_threshold:
                            word = label_map[str(predicted_idx_val)]

                            # í™”ë©´ì— ê²°ê³¼ í‘œì‹œ
                            cv2.putText(
                                frame_rgb,
                                f"{word} ({confidence_val:.2%})",
                                (10, 50),
                                cv2.FONT_HERSHEY_SIMPLEX,
                                1.5,
                                (0, 255, 0),
                                3
                            )

                            # ì‚¬ì´ë“œë°” ê²°ê³¼ ì—…ë°ì´íŠ¸
                            result_placeholder.markdown(f"## {word}")
                            confidence_placeholder.progress(confidence_val)

                            # íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€)
                            if not st.session_state['history'] or st.session_state['history'][-1] != word:
                                st.session_state['history'].append(word)
                                if len(st.session_state['history']) > 10:
                                    st.session_state['history'].pop(0)

                            history_placeholder.write(" â†’ ".join(st.session_state['history']))
                        else:
                            cv2.putText(
                                frame_rgb,
                                "ë‚®ì€ ì‹ ë¢°ë„",
                                (10, 50),
                                cv2.FONT_HERSHEY_SIMPLEX,
                                1,
                                (0, 0, 255),
                                2
                            )

                # í”„ë ˆì„ í‘œì‹œ
                video_placeholder.image(frame_rgb, channels="RGB", use_container_width=True)

        finally:
            cap.release()
            hands.close()

    else:
        st.info("ğŸ‘† 'ì‹œì‘' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ìˆ˜ì–´ í†µì—­ì„ ì‹œì‘í•˜ì„¸ìš”")

# ==================== ì•± ì‹¤í–‰ ====================
if __name__ == "__main__":
    main()
